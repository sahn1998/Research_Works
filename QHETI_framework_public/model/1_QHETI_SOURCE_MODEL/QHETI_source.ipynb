{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading in Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Standard Library Imports\n",
    "# ===============================\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# ===============================\n",
    "# Third-Party Imports\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===============================\n",
    "# Local Module Imports\n",
    "# ===============================\n",
    "from QHETI_Transformer import *\n",
    "from model_evaluation import *\n",
    "from LR_scheduler import *\n",
    "\n",
    "# ... MORE IMPORTS COMMENTED OUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Settings (Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# File Paths\n",
    "# ===============================\n",
    "source_file_path = \"\"   # specify your data source path\n",
    "OUTPUT_PATH = \"\"        # specify your output path\n",
    "\n",
    "# ===============================\n",
    "# Feature Info\n",
    "# ===============================\n",
    "quadrant_features = {\n",
    "    'Q1': [],\n",
    "    'Q2': [],\n",
    "    'Q3': [],\n",
    "    'Q4': []\n",
    "}\n",
    "\n",
    "FEATURES_DROPPED = []\n",
    "\n",
    "# ===============================\n",
    "# Model Training Config\n",
    "# ===============================\n",
    "CLASS_VAR = \"class\"         \n",
    "MINORITY_CLASS = 0          \n",
    "\n",
    "NUM_CV_FOLDS = 5\n",
    "FIRST_EPOCHS = \"\"\n",
    "LAST_EPOCHS = \"\"\n",
    "BATCH_SIZE = \"\"\n",
    "NUM_LAYERS_UNFROZEN = [10, 15, 20, 25, 30]\n",
    "\n",
    "# Layers to unfreeze during transfer learning\n",
    "NUM_LAYERS_UNFROZEN_SOURCE = []\n",
    "NUM_LAYERS_UNFROZEN_IND = []\n",
    "\n",
    "bool_remove_target = False\n",
    "augmentation_algo = \"\"   # placeholder\n",
    "\n",
    "# ===============================\n",
    "# Evaluation Metrics\n",
    "# ===============================\n",
    "EVALUATION_METRICS = [\n",
    "    \"Weighted Accuracy\", \"Sensitivity/Recall\", \"Specificity\",\n",
    "    \"Precision_class0\", \"Precision_class1\", \"Precision_avg\",\n",
    "    \"F1_class0\", \"F1_class1\", \"F1_avg\", \"auc_roc_score\",\n",
    "    \"False_Discovery_Rate\", \"False_Negative_Rate\",\n",
    "    \"False_Omission_Rate\", \"False_Positive_Rate\", \"Jaccard\"\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# Experiment Groups\n",
    "# ===============================\n",
    "pop_target_grp = [\"source\"]\n",
    "patient_grp = [\n",
    "    # Patients list removed for privacy\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load & Process Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from a file and convert it to a NumPy array (if applicable)\n",
    "def load_data(source_file_path, allow_pickle=True):\n",
    "    try:\n",
    "        # Load the .npy file; expected to contain a single dictionary object\n",
    "        data_ndarr = np.load(source_file_path, allow_pickle=allow_pickle)\n",
    "        print(f\"[INFO] Loaded object of type: {type(data_ndarr)}\")\n",
    "\n",
    "        # Extract the dictionary (assumes it's the only item in the array)\n",
    "        datadict = data_ndarr.item()\n",
    "        print(f\"[INFO] Extracted dictionary of type: {type(datadict)}\")\n",
    "\n",
    "        return datadict\n",
    "\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"[ERROR] Failed to load data from {source_file_path}\") from e\n",
    "    \n",
    "datadict = load_data(source_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_data(datadict):\n",
    "    p_ids = datadict.keys()\n",
    "    sample_size_dict = {}\n",
    "    print(\"patients: n =\", len(p_ids), end=\"\\n\\n\")\n",
    "\n",
    "    for p_id in p_ids:\n",
    "        df = datadict[p_id]\n",
    "        df.columns = df.columns.str.lower()\n",
    "        # Drop common unnecessary columns\n",
    "        df.drop(FEATURES_DROPPED, axis=1, inplace=True)\n",
    "        \n",
    "        # Convert data frame to NumPy array and cast to float32\n",
    "        df = np.asarray(df).astype(np.float32)\n",
    "        print(p_id, \"shape:\", df.shape)\n",
    "        sample_size_dict[p_id] = df.shape[0]\n",
    "\n",
    "    return sample_size_dict, p_ids, df\n",
    "\n",
    "\n",
    "sample_size_dict, p_ids, df = process_patient_data(datadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(target, data, bool_remove_target, transfer=False) -> (pd.DataFrame, pd.DataFrame):\n",
    "    dataset = pd.DataFrame()\n",
    "\n",
    "    if transfer:\n",
    "        if bool_remove_target:\n",
    "            dataset = pd.concat([data[key].copy() for key in data.keys() if key != target])\n",
    "        else:\n",
    "            dataset = pd.concat([data[key].copy() for key in data.keys()])\n",
    "    else:\n",
    "        dataset = data[target].copy()\n",
    "\n",
    "    X = dataset.drop([CLASS_VAR], axis=1)\n",
    "    Y = dataset[[CLASS_VAR]]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically load external data augmentation module\n",
    "spec_file = \"../DataAugmentation.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"DataAugmentation\", spec_file)\n",
    "balance_method = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(balance_method)\n",
    "\n",
    "def data_augmentation(train_dataset, classIndex, minorityLabel, printDebug=False):\n",
    "    # Apply augmentation (placeholder: algorithm intentionally not specified)\n",
    "    train_dataset = balance_method.augment_method(\n",
    "        train_dataset,\n",
    "        numIterations=5,\n",
    "        printDebug=printDebug,\n",
    "    )\n",
    "    \n",
    "    if printDebug:\n",
    "        print(f\"[INFO] Augmented data size: {train_dataset.shape}\")\n",
    "        print(\"~~~~~~~ Class Distribution After Augmentation ~~~~~~~\")\n",
    "        print(train_dataset[classIndex].value_counts())\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Source Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Build Source Model\n",
    "# -----------------------------------\n",
    "def build_source_model(\n",
    "    input_shape=(224, 224, 3), \n",
    "    source_learning_rate=0.0001, \n",
    "    unfreeze_base_model=False\n",
    "):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    base_model = MobileNetV2(input_tensor=inputs, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = unfreeze_base_model\n",
    "\n",
    "    # NOTE: Custom proprietary architecture is applied here (not shown for confidentiality)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    source_model = Model(inputs=inputs, outputs=outputs)\n",
    "    source_model.compile(optimizer=Adam(learning_rate=source_learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return source_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pop_target_grp[0]\n",
    "\n",
    "# Different from Individual Model as we use all of the patient data for source model training\n",
    "X, Y = split_data(\n",
    "    target=target,\n",
    "    data=datadict,\n",
    "    bool_remove_target=bool_remove_target,\n",
    "    transfer=True,\n",
    ")\n",
    "\n",
    "for layer in NUM_LAYERS_UNFROZEN:\n",
    "    number_of_layers_unfrozen = layer\n",
    "    source_evaluation_results = pd.DataFrame(columns=EVALUATION_METRICS)\n",
    "    \n",
    "    # --- K-Fold Setup ---\n",
    "    if NUM_CV_FOLDS > 1:\n",
    "        kf = \"\" # NOTE: KFold setup here\n",
    "\n",
    "    for fold in range(1, NUM_CV_FOLDS + 1):\n",
    "        if NUM_CV_FOLDS == 1:\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                X.to_numpy(), Y.to_numpy(), test_size=0.2, shuffle=True\n",
    "            )\n",
    "        else:\n",
    "            train_index, test_index = #fold_indices[fold-1]\n",
    "            X_train, X_test = X.to_numpy()[train_index], X.to_numpy()[test_index]\n",
    "            Y_train, Y_test = Y.to_numpy()[train_index], Y.to_numpy()[test_index]\n",
    "\n",
    "        # --- Combine & normalize features ---\n",
    "        df_X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "        df_Y_train = pd.DataFrame(Y_train, columns=Y.columns)\n",
    "        joined_train_dataset = df_X_train.join(df_Y_train)\n",
    "\n",
    "        # NOTE: Proprietary data augmentation method is hidden\n",
    "        # NOTE: Proprietary feature-to-image transformation (QHETI) is hidden\n",
    "\n",
    "        source_model = build_source_model() # params\n",
    "\n",
    "        # NOTE: Proprietary training function internals hidden\n",
    "        # ... Training phase 1 ...\n",
    "        # ... Fine-tuning phase ...\n",
    "        # ... Saving model and training curves ...\n",
    "        # evaluator = Evaluation(individual_model)\n",
    "        # results, confusion_matrix = evaluator.model_test(images_test, Y_test)\n",
    "        # print(confusion_matrix)\n",
    "\n",
    "        # --- Save results ---\n",
    "        output_csv = \"\"\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
