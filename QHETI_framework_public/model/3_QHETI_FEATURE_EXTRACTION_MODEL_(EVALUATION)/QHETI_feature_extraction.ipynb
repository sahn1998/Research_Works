{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db09e2c",
   "metadata": {},
   "source": [
    "# 1. Loading in Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Standard Library Imports\n",
    "# ===============================\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# ===============================\n",
    "# Third-Party Imports\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# ===============================\n",
    "# Local Module Imports\n",
    "# ===============================\n",
    "from QHETI_Transformer import *\n",
    "from QHETI_eval_pipeline.data_preparer import *\n",
    "from QHETI_eval_pipeline.model_evaluator import *\n",
    "from QHETI_eval_pipeline.evaluator import *\n",
    "from QHETI_eval_pipeline.model_processor import *\n",
    "from QHETI_eval_pipeline.feature_extractor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e4800",
   "metadata": {},
   "source": [
    "# 2. Checking if GPU is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7975f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to avoid allocating all GPU memory upfront\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✅ Using GPU: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ RuntimeError: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU found. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7bd304",
   "metadata": {},
   "source": [
    "# 3. Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK FOR CORRECT KERAS AND PANDAS VERSIONS\n",
    "print(\"keras.__version__ = \", keras.__version__) # 2.14.0\n",
    "\n",
    "# Error will occur if pandas greater than specified due to loss of backward compatibility\n",
    "# https://stackoverflow.com/questions/75953279/modulenotfounderror-no-module-named-pandas-core-indexes-numeric-using-metaflo\n",
    "# pip install \"pandas<2.0.0\"\n",
    "print(\"pd.__version__ = \", pd.__version__) # 1.5.3\n",
    "print(\"np.__version__ = \", np.__version__) # 1.24.4\n",
    "print(\"tf.__version__ = \", tf.__version__) # 2.14.0\n",
    "# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "print(\"tf.config.list_physical_devices('GPU') = \", tf.config.list_physical_devices())\n",
    "print(\"tf.test.is_built_with_cuda() = \", tf.test.is_built_with_cuda())  # True\n",
    "device = \"cuda\" if tf.test.is_built_with_cuda() else \"cpu\"\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144299c",
   "metadata": {},
   "source": [
    "# 4. Data Settings (Feature Extraction / Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# File Paths\n",
    "# ===============================\n",
    "source_file_path = \"\"   # specify your data source path\n",
    "OUTPUT_PATH = \"\"        # specify your output path\n",
    "\n",
    "# ===============================\n",
    "# Feature Info\n",
    "# ===============================\n",
    "quadrant_features = {\n",
    "    'Q1': [],\n",
    "    'Q2': [],\n",
    "    'Q3': [],\n",
    "    'Q4': []\n",
    "}\n",
    "\n",
    "FEATURES_DROPPED = []\n",
    "\n",
    "# ===============================\n",
    "# Model Training Config\n",
    "# ===============================\n",
    "CLASS_VAR = \"class\"         \n",
    "MINORITY_CLASS = 0          \n",
    "\n",
    "NUM_CV_FOLDS = 3\n",
    "FIRST_EPOCHS = \"\"\n",
    "LAST_EPOCHS = [1000]\n",
    "BATCH_SIZE = \"\"\n",
    "\n",
    "# Layers to unfreeze during transfer learning\n",
    "NUM_LAYERS_UNFROZEN_SOURCE = []\n",
    "NUM_LAYERS_UNFROZEN_IND = []\n",
    "\n",
    "bool_remove_target = False\n",
    "augmentation_algo = \"\"   # placeholder\n",
    "\n",
    "CONVENTIONAL_MODEL_TYPE_LIST = [\n",
    "    \"SVM\",\n",
    "    \"DecisionTree\",\n",
    "    \"K-NN\",\n",
    "    \"LogisticRegression\",\n",
    "    \"NaiveBayes\",\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# Evaluation Metrics\n",
    "# ===============================\n",
    "EVALUATION_METRICS = [\n",
    "    \"Weighted Accuracy\", \"Sensitivity/Recall\", \"Specificity\",\n",
    "    \"Precision_class0\", \"Precision_class1\", \"Precision_avg\",\n",
    "    \"F1_class0\", \"F1_class1\", \"F1_avg\", \"auc_roc_score\",\n",
    "    \"False_Discovery_Rate\", \"False_Negative_Rate\",\n",
    "    \"False_Omission_Rate\", \"False_Positive_Rate\", \"Jaccard\"\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# Experiment Groups\n",
    "# ===============================\n",
    "patient_grp = [\n",
    "    # Patients list removed for privacy\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# Model Discovery Function (Proprietary)\n",
    "# ===============================\n",
    "def find_all_QHETI_individual_models(QHETI_individual_model_file_path, patient_group, NUM_LAYERS_UNFROZEN_POP):\n",
    "    \"\"\"\n",
    "    Proprietary function to discover and organize available individual models \n",
    "    for each patient and layer configuration.\n",
    "\n",
    "    NOTE: Core logic hidden for confidentiality.\n",
    "    \"\"\"\n",
    "    # Initialize storage dictionary\n",
    "    QHETI_Individual_Models_list = {}\n",
    "\n",
    "    # Iterate over layer configurations\n",
    "    for num_layers in NUM_LAYERS_UNFROZEN_POP:\n",
    "        layer_key = f\"{num_layers}_unfrozen_source\"\n",
    "        layer_group_models = {}\n",
    "\n",
    "        # Iterate over patients and find models (details hidden)\n",
    "        for target_id in patient_group:\n",
    "            Model_List = {}\n",
    "            Model_path = ...  # Proprietary path construction logic\n",
    "\n",
    "            # Proprietary file discovery and filtering logic\n",
    "            # if ...:\n",
    "            #     for file_name in ...:\n",
    "            #         if file_name.endswith(\".h5\"):\n",
    "            #             Model_List[file_name] = ...\n",
    "\n",
    "            print(f\"Total number of Models in {target_id} ({layer_key}): {len(Model_List)}\")\n",
    "            layer_group_models[target_id] = Model_List\n",
    "\n",
    "        QHETI_Individual_Models_list[num_layers] = layer_group_models\n",
    "\n",
    "    return QHETI_Individual_Models_list\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Function Call (Example)\n",
    "# ===============================\n",
    "# NOTE: Proprietary arguments and file path structure hidden\n",
    "QHETI_Individual_Models_list = find_all_QHETI_individual_models(\n",
    "    ... ,  # Proprietary model base path\n",
    "    ... ,  # Proprieta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069a406",
   "metadata": {},
   "source": [
    "# 5. Load & Process Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from a file and convert it to a NumPy array (if applicable)\n",
    "def load_data(source_file_path, allow_pickle=True):\n",
    "    try:\n",
    "        # Load the .npy file; expected to contain a single dictionary object\n",
    "        data_ndarr = np.load(source_file_path, allow_pickle=allow_pickle)\n",
    "        print(f\"[INFO] Loaded object of type: {type(data_ndarr)}\")\n",
    "\n",
    "        # Extract the dictionary (assumes it's the only item in the array)\n",
    "        datadict = data_ndarr.item()\n",
    "        print(f\"[INFO] Extracted dictionary of type: {type(datadict)}\")\n",
    "\n",
    "        return datadict\n",
    "\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"[ERROR] Failed to load data from {source_file_path}\") from e\n",
    "    \n",
    "datadict = load_data(source_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_data(datadict):\n",
    "    p_ids = datadict.keys()\n",
    "    sample_size_dict = {}\n",
    "    print(\"patients: n =\", len(p_ids), end=\"\\n\\n\")\n",
    "\n",
    "    for p_id in p_ids:\n",
    "        df = datadict[p_id]\n",
    "        df.columns = df.columns.str.lower()\n",
    "        # Drop common unnecessary columns\n",
    "        df.drop(FEATURES_DROPPED, axis=1, inplace=True)\n",
    "        \n",
    "        # Convert data frame to NumPy array and cast to float32\n",
    "        df = np.asarray(df).astype(np.float32)\n",
    "        print(p_id, \"shape:\", df.shape)\n",
    "        sample_size_dict[p_id] = df.shape[0]\n",
    "\n",
    "    return sample_size_dict, p_ids, df\n",
    "\n",
    "\n",
    "sample_size_dict, p_ids, df = process_patient_data(datadict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73891b5",
   "metadata": {},
   "source": [
    "# Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013985b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create output directory (proprietary path setup hidden)\n",
    "    for NUM_LAYERS in NUM_LAYERS_UNFROZEN_SOURCE:\n",
    "        output_path = ...  # Proprietary output path construction\n",
    "        os.makedirs(..., exist_ok=True)\n",
    "\n",
    "        # Step 1: Initialize evaluator (metrics)\n",
    "        evaluator = ...  # Proprietary evaluator class initialization\n",
    "\n",
    "        # Step 2: Initialize model processor with evaluator\n",
    "        model_processor = ...  # Proprietary ModelProcessor setup\n",
    "\n",
    "        # Step 3: Set up data preparer (scaling + augmentation)\n",
    "        data_preparer = ...  # Proprietary DataPreparer initialization\n",
    "\n",
    "        # Step 4: Initialize transformer for image conversion\n",
    "        qati_transformer = ...  # Proprietary QATI transformer initialization\n",
    "\n",
    "        # Step 5: Build the model evaluator\n",
    "        model_evaluator = ...  # Proprietary ModelEvaluator initialization\n",
    "\n",
    "        # Step 6: Inject processing function for parallel execution\n",
    "        model_evaluator.process_model_fn = ...  # Proprietary processing function assignment\n",
    "\n",
    "        # Step 7: Run evaluation for patient group and model types\n",
    "        model_evaluator.evaluate_group(\n",
    "            patient_group=...,\n",
    "            CONVENTIONAL_MODEL_TYPE_LIST=...\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
