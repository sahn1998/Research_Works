{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading in Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Standard Library Imports\n",
    "# ===============================\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# ===============================\n",
    "# Third-Party Imports\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===============================\n",
    "# Local Module Imports\n",
    "# ===============================\n",
    "from QHETI_Transformer import *\n",
    "from model_evaluation import *\n",
    "from LR_scheduler import *\n",
    "\n",
    "# ... MORE IMPORTS COMMENTED OUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Checking if GPU is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to avoid allocating all GPU memory upfront\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✅ Using GPU: {[gpu.name for gpu in gpus]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ RuntimeError: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU found. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras.__version__ =  2.14.0\n",
      "pd.__version__ =  1.5.3\n",
      "np.__version__ =  1.24.4\n",
      "tf.__version__ =  2.14.0\n",
      "tf.config.list_physical_devices('GPU') =  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "tf.test.is_built_with_cuda() =  True\n",
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "# CHECK FOR CORRECT KERAS AND PANDAS VERSIONS\n",
    "print(\"keras.__version__ = \", keras.__version__) # 2.14.0\n",
    "\n",
    "# Error will occur if pandas greater than specified due to loss of backward compatibility\n",
    "# https://stackoverflow.com/questions/75953279/modulenotfounderror-no-module-named-pandas-core-indexes-numeric-using-metaflo\n",
    "# pip install \"pandas<2.0.0\"\n",
    "print(\"pd.__version__ = \", pd.__version__) # 1.5.3\n",
    "print(\"np.__version__ = \", np.__version__) # 1.24.4\n",
    "print(\"tf.__version__ = \", tf.__version__) # 2.14.0\n",
    "# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
    "print(\"tf.config.list_physical_devices('GPU') = \", tf.config.list_physical_devices())\n",
    "print(\"tf.test.is_built_with_cuda() = \", tf.test.is_built_with_cuda())  # True\n",
    "device = \"cuda\" if tf.test.is_built_with_cuda() else \"cpu\"\n",
    "print(\"device = \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Settings (Individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# File Paths\n",
    "# ===============================\n",
    "source_file_path = \"\"   # specify your data source path\n",
    "OUTPUT_PATH = \"\"        # specify your output path\n",
    "\n",
    "# ===============================\n",
    "# Feature Info\n",
    "# ===============================\n",
    "quadrant_features = {\n",
    "    'Q1': [],\n",
    "    'Q2': [],\n",
    "    'Q3': [],\n",
    "    'Q4': []\n",
    "}\n",
    "\n",
    "FEATURES_DROPPED = []\n",
    "\n",
    "# ===============================\n",
    "# Model Training Config\n",
    "# ===============================\n",
    "CLASS_VAR = \"class\"         \n",
    "MINORITY_CLASS = 0          \n",
    "\n",
    "NUM_CV_FOLDS = 3\n",
    "FIRST_EPOCHS = \"\"\n",
    "LAST_EPOCHS = [1000]\n",
    "BATCH_SIZE = \"\"\n",
    "\n",
    "# Layers to unfreeze during transfer learning\n",
    "NUM_LAYERS_UNFROZEN_SOURCE = []\n",
    "NUM_LAYERS_UNFROZEN_IND = []\n",
    "\n",
    "bool_remove_target = False\n",
    "augmentation_algo = \"\"   # placeholder\n",
    "\n",
    "# ===============================\n",
    "# Evaluation Metrics\n",
    "# ===============================\n",
    "EVALUATION_METRICS = [\n",
    "    \"Weighted Accuracy\", \"Sensitivity/Recall\", \"Specificity\",\n",
    "    \"Precision_class0\", \"Precision_class1\", \"Precision_avg\",\n",
    "    \"F1_class0\", \"F1_class1\", \"F1_avg\", \"auc_roc_score\",\n",
    "    \"False_Discovery_Rate\", \"False_Negative_Rate\",\n",
    "    \"False_Omission_Rate\", \"False_Positive_Rate\", \"Jaccard\"\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# Experiment Groups\n",
    "# ===============================\n",
    "patient_grp = [\n",
    "    # Patients list removed for privacy\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load & Process Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from a file and convert it to a NumPy array (if applicable)\n",
    "def load_data(source_file_path, allow_pickle=True):\n",
    "    try:\n",
    "        # Load the .npy file; expected to contain a single dictionary object\n",
    "        data_ndarr = np.load(source_file_path, allow_pickle=allow_pickle)\n",
    "        print(f\"[INFO] Loaded object of type: {type(data_ndarr)}\")\n",
    "\n",
    "        # Extract the dictionary (assumes it's the only item in the array)\n",
    "        datadict = data_ndarr.item()\n",
    "        print(f\"[INFO] Extracted dictionary of type: {type(datadict)}\")\n",
    "\n",
    "        return datadict\n",
    "\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"[ERROR] Failed to load data from {source_file_path}\") from e\n",
    "    \n",
    "datadict = load_data(source_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_data(datadict):\n",
    "    p_ids = datadict.keys()\n",
    "    sample_size_dict = {}\n",
    "    print(\"patients: n =\", len(p_ids), end=\"\\n\\n\")\n",
    "\n",
    "    for p_id in p_ids:\n",
    "        df = datadict[p_id]\n",
    "        df.columns = df.columns.str.lower()\n",
    "        # Drop common unnecessary columns\n",
    "        df.drop(FEATURES_DROPPED, axis=1, inplace=True)\n",
    "        \n",
    "        # Convert data frame to NumPy array and cast to float32\n",
    "        df = np.asarray(df).astype(np.float32)\n",
    "        print(p_id, \"shape:\", df.shape)\n",
    "        sample_size_dict[p_id] = df.shape[0]\n",
    "\n",
    "    return sample_size_dict, p_ids, df\n",
    "\n",
    "\n",
    "sample_size_dict, p_ids, df = process_patient_data(datadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(target, data, bool_remove_target, transfer=False) -> (pd.DataFrame, pd.DataFrame):\n",
    "    dataset = pd.DataFrame()\n",
    "\n",
    "    if transfer:\n",
    "        if bool_remove_target:\n",
    "            dataset = pd.concat([data[key].copy() for key in data.keys() if key != target])\n",
    "        else:\n",
    "            dataset = pd.concat([data[key].copy() for key in data.keys()])\n",
    "    else:\n",
    "        dataset = data[target].copy()\n",
    "\n",
    "    X = dataset.drop([CLASS_VAR], axis=1)\n",
    "    Y = dataset[[CLASS_VAR]]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically load external data augmentation module\n",
    "spec_file = \"../DataAugmentation.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"DataAugmentation\", spec_file)\n",
    "balance_method = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(balance_method)\n",
    "\n",
    "def data_augmentation(train_dataset, classIndex, minorityLabel, printDebug=False):\n",
    "    # Apply augmentation (placeholder: algorithm intentionally not specified)\n",
    "    train_dataset = balance_method.augment_method(\n",
    "        train_dataset,\n",
    "        numIterations=5,\n",
    "        printDebug=printDebug,\n",
    "    )\n",
    "    \n",
    "    if printDebug:\n",
    "        print(f\"[INFO] Augmented data size: {train_dataset.shape}\")\n",
    "        print(\"~~~~~~~ Class Distribution After Augmentation ~~~~~~~\")\n",
    "        print(train_dataset[classIndex].value_counts())\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Build Model\n",
    "# -----------------------------------\n",
    "def build_individual_model(\n",
    "    pretrained_model, \n",
    "    input_shape=(224, 224, 3), \n",
    "    individual_learning_rate=0.0001, \n",
    "    unfreeze_base_model=False\n",
    "):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    base_model = MobileNetV2(input_tensor=inputs, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = unfreeze_base_model\n",
    "\n",
    "    # NOTE: Custom proprietary architecture is applied here (not shown for confidentiality)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    individual_model = Model(inputs=inputs, outputs=outputs)\n",
    "    individual_model.compile(optimizer=Adam(learning_rate=individual_learning_rate),\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "    # NOTE: Proprietary weight transfer logic is applied here (not shown for confidentiality)\n",
    "    return individual_model\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# Build Source Model\n",
    "# -----------------------------------\n",
    "def build_source_model(\n",
    "    input_shape=(224, 224, 3), \n",
    "    source_learning_rate=0.0001, \n",
    "    unfreeze_base_model=False\n",
    "):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    base_model = MobileNetV2(input_tensor=inputs, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = unfreeze_base_model\n",
    "\n",
    "    # NOTE: Custom proprietary architecture is applied here (not shown for confidentiality)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    source_model = Model(inputs=inputs, outputs=outputs)\n",
    "    source_model.compile(optimizer=Adam(learning_rate=source_learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return source_model\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# Find Source Model\n",
    "# -----------------------------------\n",
    "def find_source_model(OUTPUT_PATH, num_layers_unfrozen):\n",
    "    model_dir = \"\"\n",
    "    pattern = \"\"\n",
    "\n",
    "    # NOTE: Custom logic for selecting models is proprietary/confidential (pattern matching kept minimal)\n",
    "    matching_files = [\n",
    "        file_name\n",
    "        for file_name in os.listdir(model_dir)\n",
    "        if file_name.endswith(\".h5\") and pattern in file_name\n",
    "    ]\n",
    "\n",
    "    return sorted(matching_files)\n",
    "\n",
    "# -----------------------------------\n",
    "# Compare Model Weights (Individual vs. Source) for double check\n",
    "# -----------------------------------\n",
    "\n",
    "def compare_all_weights(individual_model, source_model):\n",
    "    # Ensure both models have the same number of layers\n",
    "    if len(individual_model.layers) != len(source_model.layers):\n",
    "        print(f\"Models have different number of layers: \"\n",
    "              f\"individual={len(individual_model.layers)}, source={len(source_model.layers)}\")\n",
    "        return False\n",
    "\n",
    "    for i, (s_layer, t_layer) in enumerate(zip(individual_model.layers, source_model.layers)):\n",
    "        s_weights = s_layer.get_weights()\n",
    "        t_weights = t_layer.get_weights()\n",
    "\n",
    "        # If both layers have weights, compare them\n",
    "        if s_weights and t_weights:\n",
    "            if len(s_weights) != len(t_weights):\n",
    "                print(f\"Layer {i} ('{s_layer.name}') weights count mismatch \"\n",
    "                      f\"(individual: {len(s_weights)}, source: {len(t_weights)})\")\n",
    "                return False\n",
    "\n",
    "            for j, (sw, tw) in enumerate(zip(s_weights, t_weights)):\n",
    "                if not np.array_equal(sw, tw):\n",
    "                    print(f\"Layer {i} ('{s_layer.name}') weight #{j} differs.\")\n",
    "                    return False\n",
    "        # If one layer has weights and the other does not\n",
    "        elif s_weights or t_weights:\n",
    "            print(f\"Layer {i} ('{s_layer.name}') weights presence mismatch.\")\n",
    "            return False\n",
    "\n",
    "    print(\"All corresponding layers' weights match exactly.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. individual Model Run Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Train individual Model for One Fold\n",
    "# ---------------------------------------------------------\n",
    "def train_single_fold(\n",
    "    images_train, images_test, Y_train, Y_test,\n",
    "    source_model_path, layer_ind, layer_source, \n",
    "    target_id, fold, first_epochs, last_epochs, \n",
    "    patient_source_layer_dir, patient_source_layer_eval_dir,\n",
    "    version_tag\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a individual model for one fold using a source-trained model as initialization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # NOTE: Proprietary model loading, architecture, and training logic is hidden\n",
    "        # pretrained_model = build_source_model(input_shape=(224, 224, 3))\n",
    "        # pretrained_model.load_weights(source_model_path)\n",
    "        # individual_model = build_individual_model(pretrained_model=pretrained_model, input_shape=images_train.shape[1:])\n",
    "        # ... Training phase 1 ...\n",
    "        # ... Fine-tuning phase ...\n",
    "        # ... Saving model and training curves ...\n",
    "        # evaluator = Evaluation(individual_model)\n",
    "        # results, confusion_matrix = evaluator.model_test(images_test, Y_test)\n",
    "        # print(confusion_matrix)\n",
    "\n",
    "        # Placeholder return for demonstration\n",
    "        return [\"metric1\", \"metric2\", \"metric3\"]\n",
    "\n",
    "    finally:\n",
    "        # Clear session and memory (safe to keep)\n",
    "        K.clear_session()\n",
    "        # del individual_model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Patient-Specific Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Main Training Loop: Patients × Layers × Folds\n",
    "# ---------------------------------------------------------\n",
    "for last_epochs in LAST_EPOCHS:\n",
    "    for layer_source in NUM_LAYERS_UNFROZEN_source:\n",
    "        print(f\"🧪 source Model Unfrozen Layers: {layer_source}\")\n",
    "        print(f\"🧪 Last Epochs (Fine-tuning stage): {last_epochs}\")\n",
    "\n",
    "        for target_id in patient_grp:\n",
    "            # --- Create directories ---\n",
    "            # --- Split data for this target patient ---\n",
    "            X, Y = split_data(\n",
    "                target=target_id,\n",
    "                data=datadict,\n",
    "                bool_remove_target=bool_remove_target,\n",
    "                transfer=False,\n",
    "            )\n",
    "\n",
    "            for layer_ind in NUM_LAYERS_UNFROZEN_IND:\n",
    "                print(f\"🧪 individual Model Unfrozen Layers: {layer_ind}\")\n",
    "                source_evaluation_results = pd.DataFrame(columns=EVALUATION_METRICS)\n",
    "\n",
    "                # --- K-Fold Setup ---\n",
    "                if NUM_CV_FOLDS > 1:\n",
    "                    kf = \"\" # NOTE: KFold setup here\n",
    "\n",
    "                for fold in range(1, NUM_CV_FOLDS + 1):\n",
    "                    print(f\"📂 Fold: {fold}\")\n",
    "\n",
    "                    # --- Train/test split ---\n",
    "                    if NUM_CV_FOLDS == 1:\n",
    "                        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                            X.to_numpy(), Y.to_numpy(), test_size=0.2, shuffle=True\n",
    "                        )\n",
    "                    else:\n",
    "                        train_idx, test_idx = # fold_indices\n",
    "                        X_train, X_test = X.to_numpy()[train_idx], X.to_numpy()[test_idx]\n",
    "                        Y_train, Y_test = Y.to_numpy()[train_idx], Y.to_numpy()[test_idx]\n",
    "\n",
    "                    # --- Combine & normalize features ---\n",
    "                    df_X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "                    df_Y_train = pd.DataFrame(Y_train, columns=Y.columns)\n",
    "                    joined_train_dataset = df_X_train.join(df_Y_train)\n",
    "\n",
    "                    # NOTE: Proprietary data augmentation method is hidden\n",
    "                    # NOTE: Proprietary feature-to-image transformation (QHETI) is hidden\n",
    "\n",
    "                    # --- Find source-trained models ---\n",
    "                    source_model_files = find_source_model(OUTPUT_PATH, layer_source)\n",
    "\n",
    "                    for source_model_file in source_model_files:\n",
    "                        print(f\" ~~~ Patient: {target_id} | Fold: {fold} | source Model: {source_model_file} ~~~\")\n",
    "\n",
    "                        VERSION_TAG = \"\"\n",
    "                        source_model_path = \"\"\n",
    "\n",
    "                        # NOTE: Proprietary training function internals hidden\n",
    "                        result_list = train_single_fold(\n",
    "                            images_train=None,   # placeholder\n",
    "                            images_test=None,    # placeholder\n",
    "                            Y_train=Y_train,\n",
    "                            Y_test=Y_test,\n",
    "                            source_model_path=source_model_path,\n",
    "                            layer_ind=layer_ind,\n",
    "                            layer_source=layer_source,\n",
    "                            target_id=target_id,\n",
    "                            fold=fold,\n",
    "                            first_epochs=FIRST_EPOCHS,\n",
    "                            last_epochs=last_epochs,\n",
    "                            patient_source_layer_dir=\"\",        # placeholder\n",
    "                            patient_source_layer_eval_dir=\"\",   # placeholder\n",
    "                            version_tag=VERSION_TAG\n",
    "                        )\n",
    "\n",
    "                        source_evaluation_results.loc[VERSION_TAG] = result_list\n",
    "\n",
    "                        # --- Save results ---\n",
    "                        output_csv = \"\"\n",
    "                        single_result_df = pd.DataFrame(source_evaluation_results.loc[[VERSION_TAG]])\n",
    "                        if output_csv.exists():\n",
    "                            single_result_df.to_csv(output_csv, mode=\"a\", header=False)\n",
    "                        else:\n",
    "                            single_result_df.to_csv(output_csv, mode=\"a\", header=True)\n",
    "\n",
    "                        clear_output(wait=True)\n",
    "                        K.clear_session()\n",
    "                        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
